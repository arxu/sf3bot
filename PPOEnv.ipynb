{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d36c32-61eb-435b-b664-160e4c6000c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network import FeedForwardNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from MAMEToolkit.emulator import Emulator\n",
    "from MAMEToolkit.emulator import Address\n",
    "from MAMEToolkit.sf_environment.Actions import Actions\n",
    "from Steps import *\n",
    "\n",
    "class PPOEnv:\n",
    "    def __init__(self, env):\n",
    "        #Initialise hyperparameters\n",
    "        self._init_hyperparameters()\n",
    "        \n",
    "        self.render = True\n",
    "        self.frame_skip = False\n",
    "        self.sound = False\n",
    "        self.debug = False\n",
    "        self.binary_path = None\n",
    "        self.difficulty = 3\n",
    "        self.throttle = throttle\n",
    "        self.started = False\n",
    "        self.expected_health = {\"P1\": 0, \"P2\": 0}\n",
    "        self.expected_wins = {\"P1\": 0, \"P2\": 0}\n",
    "        self.round_done = False\n",
    "        self.stage_done = False\n",
    "        self.game_done = False\n",
    "        self.stage = 1\n",
    "        self.emu = Emulator(env_id, roms_path, \"sfiii3n\", setup_memory_addresses(), frame_ratio=frame_ratio, render=render, throttle=throttle, frame_skip=frame_skip, sound=sound, debug=debug, binary_path=binary_path)\n",
    "\n",
    "        #Extract environment info\n",
    "        self.env = env\n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        self.act_dim = env.action_space.shape[0]\n",
    "        \n",
    "        #ALG STEP 1\n",
    "        #Initialise actor and critic networks\n",
    "        self.actor = FeedForwardNN(self.obs_dim, self.act_dim)\n",
    "        self.critic = FeedForwardNN(self.obs_dim, 1)\n",
    "        \n",
    "        #Variable for covariance matrix\n",
    "        self.cov_var = torch.full(size = (self.act_dim,), fill_value = 0.5)\n",
    "        \n",
    "        #Create covariance matrix\n",
    "        self.cov_mat = torch.diag(self.cov_var)\n",
    "        \n",
    "        #Initialise Adam optimiser for actor and critic\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
    "        \n",
    "    def _init_hyperparameters(self):\n",
    "        self.max_timesteps_per_batch = 4800\n",
    "        self.max_timesteps_per_episode = 1600\n",
    "        self.gamma = 0.95\n",
    "        self.epochs = 5\n",
    "        self.clip = 0.2 #Recommended value from PPO original paper\n",
    "        self.lr = 0.005\n",
    "        \n",
    "        self.frame_ratio = 2\n",
    "        self.frames_per_step = 2\n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        #Get mean from actor (Identical to self.actor.forward(obs))\n",
    "        mean = self.actor(obs)\n",
    "        \n",
    "        #Create Multivariate Normal Distribution\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        \n",
    "        #Sample action from the distribution\n",
    "        action = dist.sample()\n",
    "        \n",
    "        #Get log prob from action from distribution\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        #Isolate tensors from graphs and return action and log_prob\n",
    "        return action.detach().numpy(), log_prob.detach()\n",
    "        \n",
    "    def compute_rtgs(self, batch_rews):\n",
    "        #Rewards-to-go per episode per batch to return\n",
    "        #Shape will be (number of timesteps per episode)\n",
    "        batch_rtgs = []\n",
    "        \n",
    "        #Interate through episodes backwards to maintain same order in batch_rtgs\n",
    "        for ep_rews in reversed(batch_rews):\n",
    "            discounted_reward = 0 #Discounted reward so far\n",
    "            \n",
    "            for rew in reversed(ep_rews):\n",
    "                discounted_reward = rew + discounted_reward * self.gamma\n",
    "                batch_rtgs.insert(0, discounted_reward)\n",
    "                \n",
    "        #Convert rewards-to-go to tensor\n",
    "        batch_rtgs = torch.tensor(batch_rtgs, dtype = torch.float)\n",
    "        \n",
    "        return batch_rtgs\n",
    "        \n",
    "    def learn(self, t_max):\n",
    "        #Timesteps simulated so far\n",
    "        t_now = 0\n",
    "        \n",
    "        #ALG STEP 2\n",
    "        while t_now < t_max:\n",
    "            #ALG STEP 3\n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = self.rollout()\n",
    "            \n",
    "            #Increment current timesteps by timesteps gained this batch\n",
    "            t_now += np.sum(batch_lens)\n",
    "            \n",
    "            #Calculate V_(phi, k)\n",
    "            V, _ = self.evaluate(batch_obs, batch_acts)\n",
    "            \n",
    "            #ALG STEP 5\n",
    "            #Calculate advantage\n",
    "            A_k = batch_rtgs - V.detach()\n",
    "            \n",
    "            #Normalize advantage\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10) #add 1e-10 to avoid divide-by-zero\n",
    "            \n",
    "            for _ in range(self.epochs): #ALG STEP 6 & 7\n",
    "                #Calculate V_phi and pi_theta(a_t | s_t)\n",
    "                V, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
    "                \n",
    "                #Calculate ratios\n",
    "                ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "                \n",
    "                #Calculate surrogate losses\n",
    "                surr1 = ratios * A_k #Raw ratios\n",
    "                surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k #Binds ratios within clip hyperparameter from 1\n",
    "\n",
    "                #Negative minimum of losses causes Adam optimiser to maximise loss\n",
    "                #Then get single loss by getting the mean\n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "                \n",
    "                #Calculate critic loss as mean-squared error of predictions and rewards-to-go\n",
    "                critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
    "                \n",
    "                #Calculate gradients and perform backpropagation on actor network\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward(retain_graph = True)\n",
    "                self.actor_optim.step()\n",
    "                \n",
    "                #Calculate gradients and perform backpropagation on critic network\n",
    "                self.critic_optim.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optim.step()\n",
    "            \n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        #Get predictions V for each obs in batch_obs from critic network, squeezing to reduce tensor dimensions to 1\n",
    "        V = self.critic(batch_obs).squeeze()\n",
    "        \n",
    "        #Calculate log probabilites of batch actions using most recent actor network\n",
    "        mean = self.actor(batch_obs)\n",
    "        dist = MultivariateNormal(mean, self.cov_mat)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "        \n",
    "        #Return predictions V and log probs log_probs\n",
    "        return V, log_probs\n",
    "            \n",
    "    def rollout(self):\n",
    "        #Batch data\n",
    "        batch_obs = [] #observations\n",
    "        batch_acts = [] #actions\n",
    "        batch_log_probs = [] #log probabilities of each action\n",
    "        batch_rews = [] #rewards\n",
    "        batch_rtgs = [] #rewards-to-go\n",
    "        batch_lens = [] #episodic lengths in batch\n",
    "        \n",
    "        #Timesteps simulated so far\n",
    "        t_now = 0\n",
    "        \n",
    "        while t_now < self.max_timesteps_per_batch:\n",
    "            #Rewards this episode\n",
    "            ep_rews = []\n",
    "            \n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "                #Increment timesteps ran this batch so far\n",
    "                t_now += 1\n",
    "                \n",
    "                #Collect observation\n",
    "                batch_obs.append(obs)\n",
    "                \n",
    "                action, log_prob = self.get_action(obs)\n",
    "                obs, rew, done, _ = self.env.step(action)\n",
    "                \n",
    "                #Collect reward, action and log prob\n",
    "                ep_rews.append(rew)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            #Collect episode length and reward\n",
    "            batch_lens.append(ep_t + 1)\n",
    "            batch_rews.append(ep_rews)\n",
    "        \n",
    "        #Reshape data as tensors for drawing computation graphs\n",
    "        batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "        batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
    "        batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float)\n",
    "        \n",
    "        #ALG STEP 4\n",
    "        batch_rtgs = self.compute_rtgs(batch_rews)\n",
    "        \n",
    "        #Return batch data\n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens\n",
    "\n",
    "    # Runs a set of action steps over a series of time steps\n",
    "    # Used for transitioning the emulator through non-learnable gameplay, aka. title screens, character selects\n",
    "    def run_steps(self, steps):\n",
    "        for step in steps:\n",
    "            for i in range(step[\"wait\"]):\n",
    "                self.emu.step([])\n",
    "            self.emu.step([action.value for action in step[\"actions\"]])\n",
    "\n",
    "    # Must be called first after creating this class\n",
    "    # Sends actions to the game until the learnable gameplay starts\n",
    "    # Returns the first few frames of gameplay\n",
    "    def start(self):\n",
    "        if self.throttle:\n",
    "            for i in range(int(250/self.frame_ratio)):\n",
    "                self.emu.step([])\n",
    "        self.run_steps(set_difficulty(self.frame_ratio, self.difficulty))\n",
    "        self.run_steps(start_game(self.frame_ratio))\n",
    "        frames = self.wait_for_fight_start()\n",
    "        self.started = True\n",
    "        return frames\n",
    "\n",
    "    # Observes the game and waits for the fight to start\n",
    "    def wait_for_fight_start(self):\n",
    "        data = self.emu.step([])\n",
    "        while data[\"fighting\"] == 0:\n",
    "            data = self.emu.step([])\n",
    "        self.expected_health = {\"P1\": data[\"healthP1\"], \"P2\": data[\"healthP2\"]}\n",
    "        data = self.gather_frames([])\n",
    "        return data[\"frame\"]\n",
    "\n",
    "    def reset(self):\n",
    "        if self.game_done:\n",
    "            return self.new_game()\n",
    "        elif self.stage_done:\n",
    "            return self.next_stage()\n",
    "        elif self.round_done:\n",
    "            return self.next_round()\n",
    "        else:\n",
    "            raise EnvironmentError(\"Reset called while gameplay still running\")\n",
    "\n",
    "    # To be called when a round finishes\n",
    "    # Performs the necessary steps to take the agent to the next round of gameplay\n",
    "    def next_round(self):\n",
    "        self.round_done = False\n",
    "        self.expected_health = {\"P1\": 0, \"P2\": 0}\n",
    "        return self.wait_for_fight_start()\n",
    "\n",
    "    # To be called when a game finishes\n",
    "    # Performs the necessary steps to take the agent(s) to the next game and resets the necessary book keeping variables\n",
    "    def next_stage(self):\n",
    "        self.wait_for_continue()\n",
    "        self.run_steps(next_stage(self.frame_ratio))\n",
    "        self.expected_health = {\"P1\": 0, \"P2\": 0}\n",
    "        self.expected_wins = {\"P1\": 0, \"P2\": 0}\n",
    "        self.round_done = False\n",
    "        self.stage_done = False\n",
    "        return self.wait_for_fight_start()\n",
    "\n",
    "    def new_game(self):\n",
    "        self.wait_for_continue()\n",
    "        self.run_steps(new_game(self.frame_ratio))\n",
    "        self.expected_health = {\"P1\": 0, \"P2\": 0}\n",
    "        self.expected_wins = {\"P1\": 0, \"P2\": 0}\n",
    "        self.round_done = False\n",
    "        self.stage_done = False\n",
    "        self.game_done = False\n",
    "        self.stage = 1\n",
    "        return self.wait_for_fight_start()\n",
    "\n",
    "    # Steps the emulator along until the screen goes black at the very end of a game\n",
    "    def wait_for_continue(self):\n",
    "        data = self.emu.step([])\n",
    "        if self.frames_per_step == 1:\n",
    "            while data[\"frame\"].sum() != 0:\n",
    "                data = self.emu.step([])\n",
    "        else:\n",
    "            while data[\"frame\"][0].sum() != 0:\n",
    "                data = self.emu.step([])\n",
    "\n",
    "    # Steps the emulator along until the round is definitely over\n",
    "    def run_till_victor(self, data):\n",
    "        while self.expected_wins[\"P1\"] == data[\"winsP1\"] and self.expected_wins[\"P2\"] == data[\"winsP2\"]:\n",
    "            data = add_rewards(data, self.sub_step([]))\n",
    "        self.expected_wins = {\"P1\":data[\"winsP1\"], \"P2\":data[\"winsP2\"]}\n",
    "        return data\n",
    "\n",
    "    # Checks whether the round or game has finished\n",
    "    def check_done(self, data):\n",
    "        if data[\"fighting\"] == 0:\n",
    "            data = self.run_till_victor(data)\n",
    "            self.round_done = True\n",
    "            if data[\"winsP1\"] == 2:\n",
    "                self.stage_done = True\n",
    "                self.stage += 1\n",
    "            if data[\"winsP2\"] == 2:\n",
    "                self.game_done = True\n",
    "        return data\n",
    "\n",
    "    # Collects the specified amount of frames the agent requires before choosing an action\n",
    "    def gather_frames(self, actions):\n",
    "        data = self.sub_step(actions)\n",
    "        frames = [data[\"frame\"]]\n",
    "        for i in range(self.frames_per_step - 1):\n",
    "            data = add_rewards(data, self.sub_step(actions))\n",
    "            frames.append(data[\"frame\"])\n",
    "        data[\"frame\"] = frames[0] if self.frames_per_step == 1 else frames\n",
    "        return data\n",
    "\n",
    "    # Steps the emulator along by one time step and feeds in any actions that require pressing\n",
    "    # Takes the data returned from the step and updates book keeping variables\n",
    "    def sub_step(self, actions):\n",
    "        data = self.emu.step([action.value for action in actions])\n",
    "\n",
    "        p1_diff = (self.expected_health[\"P1\"] - data[\"healthP1\"])\n",
    "        p2_diff = (self.expected_health[\"P2\"] - data[\"healthP2\"])\n",
    "        self.expected_health = {\"P1\": data[\"healthP1\"], \"P2\": data[\"healthP2\"]}\n",
    "\n",
    "        rewards = {\n",
    "            \"P1\": (p2_diff-p1_diff),\n",
    "            \"P2\": (p1_diff-p2_diff)\n",
    "        }\n",
    "\n",
    "        data[\"rewards\"] = rewards\n",
    "        return data\n",
    "\n",
    "    # Steps the emulator along by the requested amount of frames required for the agent to provide actions\n",
    "    def step(self, move_action, attack_action):\n",
    "        if self.started:\n",
    "            if not self.round_done and not self.stage_done and not self.game_done:\n",
    "                actions = []\n",
    "                actions += index_to_move_action(move_action)\n",
    "                actions += index_to_attack_action(attack_action)\n",
    "                data = self.gather_frames(actions)\n",
    "                data = self.check_done(data)\n",
    "                return data[\"frame\"], data[\"rewards\"], self.round_done, self.stage_done, self.game_done\n",
    "            else:\n",
    "                raise EnvironmentError(\"Attempted to step while characters are not fighting\")\n",
    "        else:\n",
    "            raise EnvironmentError(\"Start must be called before stepping\")\n",
    "\n",
    "    # Safely closes emulator\n",
    "    def close(self):\n",
    "        self.emu.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef36a5-837f-4ca0-8c6e-e575bdeca38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
